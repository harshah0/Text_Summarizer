{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "import re\n",
        "import heapq\n",
        "\n",
        "# Download required NLTK data\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize, sent_tokenize\n",
        "\n",
        "# Input: Long text\n",
        "print(\"Enter a long paragraph or article (end with ENTER):\")\n",
        "input_text = input()\n",
        "\n",
        "if input_text.strip() == \"\":\n",
        "    print(\"\\n Please enter some text.\")\n",
        "else:\n",
        "    # Step 1: Clean text\n",
        "    text = re.sub(r'\\s+', ' ', input_text)\n",
        "    clean_text = re.sub(r'[^a-zA-Z]', ' ', text).lower()\n",
        "\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    words = word_tokenize(clean_text)\n",
        "\n",
        "    # Step 2: Create frequency table\n",
        "    word_frequencies = {}\n",
        "    for word in words:\n",
        "        if word not in stop_words:\n",
        "            word_frequencies[word] = word_frequencies.get(word, 0) + 1\n",
        "\n",
        "    # Step 3: Normalize frequencies\n",
        "    max_freq = max(word_frequencies.values())\n",
        "    for word in word_frequencies:\n",
        "        word_frequencies[word] /= max_freq\n",
        "\n",
        "    # Step 4: Score sentences\n",
        "    sentences = sent_tokenize(text)\n",
        "    sentence_scores = {}\n",
        "    for sent in sentences:\n",
        "        for word in word_tokenize(sent.lower()):\n",
        "            if word in word_frequencies:\n",
        "                if len(sent.split(' ')) < 30:\n",
        "                    sentence_scores[sent] = sentence_scores.get(sent, 0) + word_frequencies[word]\n",
        "\n",
        "    # Step 5: Select top sentences\n",
        "    summary_sentences = heapq.nlargest(3, sentence_scores, key=sentence_scores.get)\n",
        "    summary = ' '.join(summary_sentences)\n",
        "\n",
        "    # Output summary\n",
        "    print(\"\\n Summary:\\n\")\n",
        "    print(summary)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "mH7zrfEcdvRP",
        "outputId": "060a7ba0-d565-404f-ea8b-5725ca7f5d3b"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter a long paragraph or article (end with ENTER):\n",
            "Artificial Intelligence (AI) is one of the most transformative forces of our time. It is reshaping industries and redefining the way humans interact with technology. AI enables machines to simulate human intelligence and perform tasks that normally require human cognition. From healthcare and finance to transportation and entertainment, AI applications are everywhere. Machine learning, a subset of AI, allows systems to learn from data and improve over time. Deep learning, which mimics the human brain through neural networks, has led to breakthroughs in image and speech recognition. Natural Language Processing (NLP) allows machines to understand and generate human language. AI-driven assistants like Siri, Alexa, and Google Assistant rely on NLP to respond to voice commands. Autonomous vehicles use AI to interpret sensor data and make driving decisions in real time. AI in healthcare is revolutionizing diagnostics, drug discovery, and personalized treatment. In finance, AI helps detect fraud, manage risks, and automate trading. Retailers use AI for inventory management, customer personalization, and sales forecasting. Manufacturers leverage AI to predict equipment failure and optimize supply chains. AI is also used in agriculture to monitor crop health and automate irrigation. AI-powered robots are performing surgeries with incredible precision. Recommendation engines, like those on Netflix and Amazon, are powered by AI algorithms. Social media platforms use AI to filter content and detect inappropriate behavior. AI has the potential to solve some of the world's most pressing challenges. Despite its benefits, AI also raises ethical concerns about bias and privacy. Data is the fuel that powers AI models, and the quality of data determines accuracy. Poor-quality or biased data can lead to flawed AI systems that discriminate. Regulations and ethical frameworks are needed to guide AI development. Explainability in AI refers to the ability to understand how a model makes decisions. Trustworthy AI systems must be transparent, fair, and accountable. AI governance includes rules, policies, and processes to manage risks and ensure compliance. AI is not a magic solution and should be used thoughtfully and responsibly. The rise of AI will impact the workforce, potentially displacing some jobs. At the same time, AI will create new job opportunities and industries. Reskilling and upskilling workers will be essential for the AI-driven future. Collaboration between humans and AI will unlock new levels of productivity. Augmented Intelligence is a model where AI assists human decision-making. AI in education can provide personalized learning experiences and tutoring. In cybersecurity, AI helps detect threats and respond to attacks in real time. AI can support environmental sustainability by optimizing energy use. Smart cities use AI to manage traffic, waste, and public safety. AI models must be trained with diverse and inclusive datasets. Bias in AI can amplify societal inequalities if not properly addressed. Open-source AI tools have democratized access to powerful technologies. Cloud computing has made it easier to deploy and scale AI solutions. Edge AI enables data processing directly on devices, reducing latency. AI chips and hardware accelerators are improving performance and efficiency. Human oversight is essential to ensure AI systems align with human values. Interdisciplinary research is key to advancing AI safely and effectively. AI is being integrated into IoT devices for smarter homes and industries. Emotion AI aims to recognize and respond to human emotions. Generative AI can create text, images, and music from scratch. Large Language Models (LLMs) like GPT are changing the way we write and code. AI in gaming is making characters more realistic and immersive. Simulations powered by AI are used for training, testing, and research. AI can help governments make data-driven policy decisions. Remote sensing with AI aids in disaster response and climate monitoring. AI is enabling breakthroughs in genomics and molecular biology. In legal tech, AI helps review contracts and analyze case law. Virtual reality and AI together offer new experiences in entertainment and training. AI can support mental health by providing chatbots and mood analysis. The scalability of AI makes it suitable for both small startups and large enterprises. Quantum computing may further accelerate AI capabilities in the future. Ethical AI development requires multidisciplinary collaboration. Transparency reports help track AI model behavior and impacts. Synthetic data is used to train AI without compromising privacy. AI systems need regular evaluation and auditing for fairness. Bias mitigation techniques are crucial in sensitive domains like hiring and lending. Federated learning allows training AI models without sharing raw data. AI literacy is important for society to engage with and govern technology. AI competitions and challenges are pushing the boundaries of innovation. Bias audits and red-teaming are used to stress-test AI models. Digital twins powered by AI simulate real-world systems for optimization. Speech-to-text and text-to-speech models enhance accessibility. AI models need to be monitored for drift and performance degradation. Self-supervised learning reduces the need for labeled data. Reinforcement learning teaches agents to act through trial and error. AI in logistics optimizes routes, reduces delays, and cuts costs. AI translators break language barriers and connect global communities. Hybrid models combine symbolic AI with neural networks. Ethical concerns also include AI-generated misinformation and deepfakes. AI governance should be proactive, not reactive. AI can help monitor and predict pandemics using real-time data. As AI evolves, so too must our legal, social, and educational systems. The balance between innovation and regulation is delicate but vital. AI ethics boards and review panels should be independent and diverse. AI is not inherently good or bad—it reflects how we choose to use it. We must ensure AI benefits are shared equitably across society. AI should augment human potential, not replace it entirely. The future of AI depends on the choices we make today. Education systems must teach AI awareness from an early age. Open dialogue and public engagement are necessary for responsible AI. Developing countries can leapfrog using AI in key sectors. Digital infrastructure and access must be improved to support AI adoption. Global collaboration is essential to address cross-border AI challenges. Trust in AI must be earned through responsible practices. Inclusive innovation ensures that AI works for everyone. AI should serve humanity — ethically, responsibly, and wisely.\n",
            "\n",
            " Summary:\n",
            "\n",
            "Machine learning, a subset of AI, allows systems to learn from data and improve over time. Autonomous vehicles use AI to interpret sensor data and make driving decisions in real time. Data is the fuel that powers AI models, and the quality of data determines accuracy.\n"
          ]
        }
      ]
    }
  ]
}